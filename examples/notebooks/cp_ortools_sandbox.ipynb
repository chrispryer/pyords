{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or-tools Implementation Development Sandbox\n",
    "The goal of this notebook is to establish an effective workflow for model engineering. \n",
    "\n",
    "  - Data Read\n",
    "  - Preprocessing\n",
    "    - geocoding\n",
    "    - distance processing\n",
    "    - clustering\n",
    "    - additional configuration\n",
    "  - Modeling\n",
    "  - Post-processing\n",
    "    - Scoring\n",
    "    \n",
    "Each of these components will be fleshed out from this notebook and refactored into a newer version of the pyords library. The current workflow is as follow:\n",
    "\n",
    "1. init notebook\n",
    "2. script task\n",
    "3. test task\n",
    "4. complete objective\n",
    "5. refactor into library using the same (if not more) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ortools --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as pe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "this_dir = os.path.abspath('')\n",
    "root_dir = os.path.dirname(this_dir)\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading input shipment data\n",
    "Geocoding has been completed already. For initial versions scope is limited to contiguous US 5-digit zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../tests/vrp_testing_data.csv')\n",
    "\n",
    "required_cols = ['weight', 'pallets', 'zipcode']\n",
    "for col in required_cols: assert col in df.columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(dataframe:pd.DataFrame, colors:str=None):\n",
    "    return pe.scatter_geo(\n",
    "        dataframe, \n",
    "        lat='latitude', \n",
    "        lon='longitude', \n",
    "        size='pallets',\n",
    "        color=colors,\n",
    "        scope='usa'\n",
    "    )\n",
    "\n",
    "get_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Distance processing\n",
    "For the ortools model setup the program needs to be fed a distance matrix that includes an origin node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "\n",
    "# select an origin node\n",
    "origin_lon, origin_lat = 41.4191, -87.7748\n",
    "origins = [(origin_lon, origin_lat)]\n",
    "\n",
    "distances = []\n",
    "for coords0 in origins + list(zip(df.latitude, df.longitude)):\n",
    "    row = []\n",
    "    for coords1 in origins + list(zip(df.latitude, df.longitude)):\n",
    "        distance = haversine(coords0, coords1, unit=Unit.MILES)\n",
    "        row.append(distance)\n",
    "    distances.append(row)\n",
    "\n",
    "assert len(distances) == len(origins) + len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "As part of the modeling process we'll improve how we are preparing the data. To nail down effective logic to implement, I'll look to add processing here for **clustering** and **cluster improvements**.\n",
    "By clustering we can segment the problem up into realistic problem spaces. So far DBSCAN with ad-hoc mile-constrained chaining is used to pluck out nodes close enough together to even consider for one route (TODO: needs improvement).\n",
    "Once we have these clusters we can allow for additional flexibility in the overall solution by tweaking the segmentation determined by DBSCAN. Setting a fixed constraint of X miles might work initially, but there could be return routes, connecting stops, etc. that make one-offs more appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or-tools modeling\n",
    "At this point the goal is to utilize ortools' cvrp wrappers. We are going to design this as a capacitated vrp without time windows or any complex penalties for the initial implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "\n",
    "# inputs\n",
    "# - distance matrix: graph defined as matrix \n",
    "#  (point to point; len(matrix) == number of unique nodes)\n",
    "# - vehicle list: [max_cap, max_cap, max_cap] defined in units of demand\n",
    "# - demand list: [units, units, units, ... n_distances]\n",
    "# - max_solve_seconds\n",
    "# - depot_index: poisition of node in definitions to use as origin node\n",
    "\n",
    "# process vehicles input using total destination nodes count\n",
    "# i.e. one truck available per stop\n",
    "vehicles = [26 for i in range(len(distances[1:]))]\n",
    "\n",
    "demand = np.insert(df.pallets.values, 0, 0) # using pallets & adding 0 for the depot\n",
    "max_solve_seconds = 30\n",
    "depot_index = 0\n",
    "\n",
    "# constructing the model\n",
    "manager = pywrapcp.RoutingIndexManager(len(distances), len(vehicles), 0)\n",
    "\n",
    "def distance_callback(i:int, j:int):\n",
    "    \"\"\"index of from (i) and to (j)\"\"\"\n",
    "    node_i = manager.IndexToNode(i)\n",
    "    node_j = manager.IndexToNode(j)\n",
    "    \n",
    "    return distances[node_i][node_j]\n",
    "\n",
    "model = pywrapcp.RoutingModel(manager)\n",
    "\n",
    "# add distance constraint\n",
    "model.SetArcCostEvaluatorOfAllVehicles(\n",
    "    model.RegisterTransitCallback(distance_callback)\n",
    ")\n",
    "\n",
    "def demand_callback(i:int):\n",
    "    \"\"\"capacity constraint\"\"\"\n",
    "    node = manager.IndexToNode(i)\n",
    "    \n",
    "    return demand[node]\n",
    "\n",
    "# add demand constraint\n",
    "model.AddDimensionWithVehicleCapacity(\n",
    "    # function which return the load at each location (cf. cvrp.py example)\n",
    "    model.RegisterUnaryTransitCallback(demand_callback),\n",
    "    0, # null capacity slack\n",
    "    np.array([cap for cap in vehicles]), # vehicle maximum capacity\n",
    "    True, # start cumul to zero\n",
    "    'Capacity'\n",
    ")\n",
    "\n",
    "# config for optimization search\n",
    "search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "search_parameters.first_solution_strategy = \\\n",
    "    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
    "search_parameters.time_limit.seconds = max_solve_seconds\n",
    "\n",
    "assignment = model.SolveWithParameters(search_parameters)\n",
    "\n",
    "def get_solution():\n",
    "    total_distance = 0\n",
    "    total_load = 0\n",
    "    solution = []\n",
    "    for vehicle in range(len(vehicles)):\n",
    "        i = model.Start(vehicle)\n",
    "        info = {'vehicle': vehicle, 'stops': list(), 'stop_distances': [0],\n",
    "                'stop_loads': list()}\n",
    "\n",
    "        while not model.IsEnd(i):\n",
    "            node = manager.IndexToNode(i)\n",
    "            info['stops'].append(node)\n",
    "            info['stop_loads'].append(demand[node])\n",
    "            \n",
    "            previous_i = int(i)\n",
    "            i = assignment.Value(model.NextVar(i))\n",
    "            info['stop_distances'].append(model.GetArcCostForVehicle(previous_i, i, vehicle))\n",
    "        \n",
    "        # add return to depot to align with solution data\n",
    "        info['stops'].append(0)\n",
    "        info['stop_loads'].append(0)\n",
    "        solution.append(info)\n",
    "        \n",
    "    return solution\n",
    "\n",
    "solution = get_solution()\n",
    "\n",
    "assert len(solution) > 0 # TODO: create better solution testing\n",
    "\n",
    "vehicleindex_w_moststops = np.argmax([len(v['stops']) for v in solution])\n",
    "vehicles_w_loads = [v for v in solution if sum(v['stop_loads']) > 0]\n",
    "print('total vehicles: %s' % len(solution))\n",
    "print('total vehicles w loads: %s' % len(vehicles_w_loads))\n",
    "#print('total load: %s' % solution[-1])\n",
    "print('total input load: %s' % demand.sum())\n",
    "print('max stop sequence: %s' % solution[vehicleindex_w_moststops]['stops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "**Scoring** a solution against a standardized formula/method will allow for more comprehensive testing, debugging, and model tuning. Scoring should be broken down into a **theory score** and a **practical score**. Theory scores will utilize theoretical expectations of vrp solutions (some assumptions about implementations needs to be made). The practical score will measure how implementable a solution is with respect to certain real-world expectations.  \n",
    "\n",
    "For the sake of simplicity I'll use pandas as my target format and align scoring to get functions that pull from standard solution structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_load_factor(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_solution_to_dataframe(solution:list, dataframe:pd.DataFrame):\n",
    "    for v in solution:\n",
    "        # accounting for insert of origin to matrix input\n",
    "        stops = list(np.array(v['stops'][1:-1]) - 1)\n",
    "\n",
    "        dataframe.loc[stops, 'vehicle'] = str(v['vehicle'])\n",
    "        dataframe.loc[stops, 'sequence'] = list(range(len(stops))) # assumes order matches\n",
    "        dataframe.loc[stops, 'stop_distance'] = v['stop_distances'][1:-1]\n",
    "        dataframe.loc[stops, 'stop_loads'] = v['stop_loads'][1:-1]\n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "# scoring theoretical\n",
    "# average capacity utilization of vehicles\n",
    "def get_load_factor(solution:list):\n",
    "    total_loads = sum([sum(s['stop_loads']) for s in solution])\n",
    "    total_utilized_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "    \n",
    "    return total_loads/total_utilized_vehicles\n",
    "\n",
    "def score_load_factor(dataframe:pd.DataFrame):\n",
    "    return dataframe.groupby('vehicle').pallets.sum().mean()\n",
    "\n",
    "# average distance traveled per vehicle\n",
    "# NOTE: excluding distances returning to depot for now\n",
    "# need to refactor for this.\n",
    "def get_distance_factor(solution:list):\n",
    "    total_distances = sum([sum(s['stop_distances'][:-1]) for s in solution])\n",
    "    total_utilized_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "    \n",
    "    return total_distances/total_utilized_vehicles\n",
    "\n",
    "def score_distance_factor(dataframe:pd.DataFrame):\n",
    "    return dataframe.groupby('vehicle').stop_distance.sum().mean()\n",
    "\n",
    "# average distance per stop\n",
    "def score_travel_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# ratio of one-stop routes to multi-stop \n",
    "# (assumption is that implementation is looking for multi-stops)\n",
    "def score_multistop_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# general service measured in total capacity serviced over total in scope\n",
    "def score_multistop_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# scoring practice\n",
    "# deviation/distribution of stop distances per route\n",
    "def score_erratic_distance_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# measuring total number of moves across state boarders\n",
    "def score_state_crossing_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "df = process_solution_to_dataframe(solution, df)\n",
    "\n",
    "load_factor = score_load_factor(df)\n",
    "assert load_factor == get_load_factor(solution)\n",
    "\n",
    "distance_factor = score_distance_factor(df)\n",
    "assert distance_factor == get_distance_factor(solution)\n",
    "\n",
    "stop_travel_factor = score_travel_factor(df)\n",
    "multistop_factor = score_multistop_factor(df)\n",
    "satisfaction_factor = score_multistop_factor(df)\n",
    "erratic_distance_factor = score_erratic_distance_factor(df)\n",
    "crossstate_factor = score_state_crossing_factor(df)\n",
    "\n",
    "print('load_factor:', load_factor)\n",
    "print('distance_factor:', distance_factor)\n",
    "print('stop_travel_factor:', stop_travel_factor)\n",
    "print('multistop_factor:', multistop_factor)\n",
    "print('satisfaction_factor:', satisfaction_factor)\n",
    "print('erratic_distance_factor:', erratic_distance_factor)\n",
    "print('crossstate_factor:', crossstate_factor)\n",
    "get_plot(df, 'vehicle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
