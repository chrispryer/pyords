{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or-tools Implementation Development Sandbox\n",
    "The goal of this notebook is to establish an effective workflow for model engineering. \n",
    "\n",
    "  - Data Read\n",
    "  - Preprocessing\n",
    "    - geocoding\n",
    "    - distance processing\n",
    "    - clustering\n",
    "    - additional configuration\n",
    "  - Modeling\n",
    "  - Post-processing\n",
    "    - Scoring\n",
    "    \n",
    "Each of these components will be fleshed out from this notebook and refactored into a newer version of the pyords library. The current workflow is as follow:\n",
    "\n",
    "1. init notebook\n",
    "2. script task\n",
    "3. test task\n",
    "4. complete objective\n",
    "5. refactor into library using the same (if not more) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ortools --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as pe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "this_dir = os.path.abspath('')\n",
    "root_dir = os.path.dirname(this_dir)\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading input shipment data\n",
    "Geocoding has been completed already. For initial versions scope is limited to contiguous US 5-digit zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../tests/vrp_testing_data.csv')\n",
    "\n",
    "required_cols = ['weight', 'pallets', 'zipcode']\n",
    "for col in required_cols: assert col in df.columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(dataframe:pd.DataFrame, colors:str=None):\n",
    "    return pe.scatter_geo(\n",
    "        dataframe, \n",
    "        lat='latitude', \n",
    "        lon='longitude', \n",
    "        size='pallets',\n",
    "        color=colors,\n",
    "        scope='usa'\n",
    "    )\n",
    "\n",
    "get_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Distance processing\n",
    "For the ortools model setup the program needs to be fed a distance matrix that includes an origin node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "\n",
    "def get_distance_matrix_from_dataframe(origins:list, dataframe:pd.DataFrame):\n",
    "    # select an origin node\n",
    "\n",
    "    distances = []\n",
    "    for coords0 in origins + list(zip(dataframe.latitude, dataframe.longitude)):\n",
    "        row = []\n",
    "        for coords1 in origins + list(zip(df.latitude, df.longitude)):\n",
    "            distance = haversine(coords0, coords1, unit=Unit.MILES)\n",
    "            row.append(distance)\n",
    "        distances.append(row)\n",
    "        \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "As part of the modeling process we'll improve how we are preparing the data. To nail down effective logic to implement, I'll look to add processing here for **clustering** and **cluster improvements**.\n",
    "By clustering we can segment the problem up into realistic problem spaces. So far DBSCAN with ad-hoc mile-constrained chaining is used to pluck out nodes close enough together to even consider for one route (TODO: needs improvement).\n",
    "Once we have these clusters we can allow for additional flexibility in the overall solution by tweaking the segmentation determined by DBSCAN. Setting a fixed constraint of X miles might work initially, but there could be return routes, connecting stops, etc. that make one-offs more appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or-tools modeling\n",
    "At this point the goal is to utilize ortools' cvrp wrappers. We are going to design this as a capacitated vrp without time windows or any complex penalties for the initial implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "\n",
    "# inputs\n",
    "# - distance matrix: graph defined as matrix \n",
    "#  (point to point; len(matrix) == number of unique nodes)\n",
    "# - vehicle list: [max_cap, max_cap, max_cap] defined in units of demand\n",
    "# - demand list: [units, units, units, ... n_distances]\n",
    "# - max_solve_seconds\n",
    "# - depot_index: poisition of node in definitions to use as origin node\n",
    "\n",
    "# process vehicles input using total destination nodes count\n",
    "# i.e. one truck available per stop\n",
    "\n",
    "\n",
    "# constructing the model\n",
    "def get_manager(distances:list, vehicles:list, depot_index:int): \n",
    "    return pywrapcp.RoutingIndexManager(len(distances), len(vehicles), 0)\n",
    "\n",
    "def get_model(manager):\n",
    "    return pywrapcp.RoutingModel(manager)\n",
    "\n",
    "# config for optimization search\n",
    "def get_search_params(max_solve_seconds:int=30):\n",
    "    search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_parameters.first_solution_strategy = \\\n",
    "        routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
    "    search_parameters.time_limit.seconds = max_solve_seconds\n",
    "    \n",
    "    return search_parameters\n",
    "\n",
    "class MyPipe:\n",
    "    def __init__(self, manager, model):\n",
    "        self.manager = manager\n",
    "        self.model = model\n",
    "        self.distances = None\n",
    "        self.vehicles = None\n",
    "        self.demand = None\n",
    "        \n",
    "    def distance_callback(self, i:int, j:int):\n",
    "        \"\"\"index of from (i) and to (j)\"\"\"\n",
    "        node_i = self.manager.IndexToNode(i)\n",
    "        node_j = self.manager.IndexToNode(j)\n",
    "\n",
    "        return self.distances[node_i][node_j]\n",
    "    \n",
    "    def set_distance_callback(self):\n",
    "        self.model.SetArcCostEvaluatorOfAllVehicles(\n",
    "            self.model.RegisterTransitCallback(self.distance_callback)\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_distances(self, distances):\n",
    "        self.distances = distances\n",
    "        self.set_distance_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def demand_callback(self, i:int):\n",
    "        \"\"\"capacity constraint\"\"\"\n",
    "        node = self.manager.IndexToNode(i)\n",
    "\n",
    "        return self.demand[node]\n",
    "    \n",
    "    def set_demand_callback(self):\n",
    "        # add demand constraint using vehicles\n",
    "        self.model.AddDimensionWithVehicleCapacity(\n",
    "            # function which return the load at each location (cf. cvrp.py example)\n",
    "            self.model.RegisterUnaryTransitCallback(self.demand_callback),\n",
    "            0, # null capacity slack\n",
    "            np.array([cap for cap in self.vehicles]), # vehicle maximum capacity\n",
    "            True, # start cumul to zero\n",
    "            'Capacity'\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_vehicles(self, vehicles:list):\n",
    "        self.vehicles = vehicles\n",
    "        if self.demand: \n",
    "            return self.set_demand_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_demand(self, demand:list):\n",
    "        self.demand = demand\n",
    "        if self.vehicles: \n",
    "            return self.set_demand_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_solution(self, assignment):\n",
    "        total_distance = 0\n",
    "        total_load = 0\n",
    "        solution = []\n",
    "        for vehicle in range(len(self.vehicles)):\n",
    "            i = self.model.Start(vehicle)\n",
    "            info = {'vehicle': vehicle, 'stops': list(), 'stop_distances': [0],\n",
    "                    'stop_loads': list()}\n",
    "\n",
    "            while not self.model.IsEnd(i):\n",
    "                node = self.manager.IndexToNode(i)\n",
    "                info['stops'].append(node)\n",
    "                info['stop_loads'].append(self.demand[node])\n",
    "\n",
    "                previous_i = int(i)\n",
    "                i = assignment.Value(self.model.NextVar(i))\n",
    "                info['stop_distances'].append(self.model.GetArcCostForVehicle(previous_i, i, vehicle))\n",
    "\n",
    "            # add return to depot to align with solution data\n",
    "            info['stops'].append(0)\n",
    "            info['stop_loads'].append(0)\n",
    "            solution.append(info)\n",
    "        \n",
    "        return solution\n",
    "            \n",
    "    def run(self, search_params):\n",
    "        return self.model.SolveWithParameters(search_params)\n",
    "    \n",
    "def get_solution_from_dataframe(dataframe:pd.DataFrame):\n",
    "    # TODO: abstraction & testing\n",
    "    origins = [(41.4191, -87.7748)] # assumed depot location for one-depot solutions\n",
    "    distances = get_distance_matrix_from_dataframe(origins, dataframe)\n",
    "    assert len(distances) == len(origins) + len(dataframe)\n",
    "    \n",
    "    vehicles = [26 for i in range(len(distances[1:]))]\n",
    "    demand = np.insert(dataframe.pallets.values, 0, 0) # using pallets & adding 0 for the depot\n",
    "    max_solve_seconds = 30\n",
    "    depot_index = 0\n",
    "    \n",
    "    manager = get_manager(distances, vehicles, depot_index)\n",
    "    model = get_model(manager)\n",
    "    pipe = MyPipe(manager, model)\n",
    "    search_params = get_search_params()\n",
    "    assignment = pipe.add_distances(distances).add_vehicles(vehicles)\\\n",
    "        .add_demand(demand)\\\n",
    "        .run(search_params) \n",
    "    \n",
    "    return pipe.get_solution(assignment)\n",
    "        \n",
    "solution = get_solution_from_dataframe(df)\n",
    "\n",
    "assert len(solution) > 0 # TODO: create better solution testing\n",
    "\n",
    "vehicleindex_w_moststops = np.argmax([len(v['stops']) for v in solution])\n",
    "vehicles_w_loads = [v for v in solution if sum(v['stop_loads']) > 0]\n",
    "print('total vehicles: %s' % len(solution))\n",
    "print('total vehicles w loads: %s' % len(vehicles_w_loads))\n",
    "#print('total load: %s' % solution[-1])\n",
    "#print('total input load: %s' % demand.sum())\n",
    "print('max stop sequence: %s' % solution[vehicleindex_w_moststops]['stops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "**Scoring** a solution against a standardized formula/method will allow for more comprehensive testing, debugging, and model tuning. Scoring should be broken down into a **theory score** and a **practical score**. Theory scores will utilize theoretical expectations of vrp solutions (some assumptions about implementations needs to be made). The practical score will measure how implementable a solution is with respect to certain real-world expectations.  \n",
    "\n",
    "For the sake of simplicity I'll use pandas as my target format and align scoring to get functions that pull from standard solution structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_solution_to_dataframe(solution:list, dataframe:pd.DataFrame):\n",
    "    for v in solution:\n",
    "        # accounting for insert of origin to matrix input\n",
    "        stops = list(np.array(v['stops'][1:-1]) - 1)\n",
    "\n",
    "        dataframe.loc[stops, 'vehicle'] = str(v['vehicle'])\n",
    "        dataframe.loc[stops, 'sequence'] = list(range(len(stops))) # assumes order matches\n",
    "        dataframe.loc[stops, 'stop_distance'] = v['stop_distances'][1:-1]\n",
    "        dataframe.loc[stops, 'stop_loads'] = v['stop_loads'][1:-1]\n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "# scoring theoretical\n",
    "# average capacity utilization of vehicles\n",
    "def get_load_factor(solution:list):\n",
    "    total_loads = sum([sum(s['stop_loads']) for s in solution])\n",
    "    total_utilized_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "    \n",
    "    return total_loads/total_utilized_vehicles\n",
    "\n",
    "def score_load_factor(dataframe:pd.DataFrame):\n",
    "    return dataframe.groupby('vehicle').pallets.sum().mean()\n",
    "\n",
    "# average distance traveled per vehicle\n",
    "# NOTE: excluding distances returning to depot for now\n",
    "# need to refactor for this.\n",
    "def get_distance_factor(solution:list):\n",
    "    total_distances = sum([sum(s['stop_distances'][:-1]) for s in solution])\n",
    "    total_utilized_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "    \n",
    "    return total_distances/total_utilized_vehicles\n",
    "\n",
    "def score_distance_factor(dataframe:pd.DataFrame):\n",
    "    return dataframe.groupby('vehicle').stop_distance.sum().mean()\n",
    "\n",
    "# average distance per stop\n",
    "def score_travel_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# ratio of one-stop routes to multi-stop \n",
    "# (assumption is that implementation is looking for multi-stops)\n",
    "def score_multistop_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# general service measured in total capacity serviced over total in scope\n",
    "def score_multistop_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# scoring practice\n",
    "# deviation/distribution of stop distances per route\n",
    "def score_erratic_distance_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "# measuring total number of moves across state boarders\n",
    "def score_state_crossing_factor(dataframe:pd.DataFrame):\n",
    "    return None\n",
    "\n",
    "def postprocess_dataframe(dataframe:pd.DataFrame):\n",
    "    # TODO: abstraction & testing\n",
    "\n",
    "    load_factor = score_load_factor(dataframe)\n",
    "    assert load_factor == get_load_factor(solution)\n",
    "\n",
    "    distance_factor = score_distance_factor(dataframe)\n",
    "    assert distance_factor == get_distance_factor(solution)\n",
    "\n",
    "    stop_travel_factor = score_travel_factor(dataframe)\n",
    "    multistop_factor = score_multistop_factor(dataframe)\n",
    "    satisfaction_factor = score_multistop_factor(dataframe)\n",
    "    erratic_distance_factor = score_erratic_distance_factor(dataframe)\n",
    "    crossstate_factor = score_state_crossing_factor(dataframe)\n",
    "\n",
    "    print('load_factor:', load_factor)\n",
    "    print('distance_factor:', distance_factor)\n",
    "    print('stop_travel_factor:', stop_travel_factor)\n",
    "    print('multistop_factor:', multistop_factor)\n",
    "    print('satisfaction_factor:', satisfaction_factor)\n",
    "    print('erratic_distance_factor:', erratic_distance_factor)\n",
    "    print('crossstate_factor:', crossstate_factor)\n",
    "    \n",
    "    return get_plot(dataframe, 'vehicle')\n",
    "\n",
    "df = process_solution_to_dataframe(solution, df)\n",
    "postprocess_dataframe(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
