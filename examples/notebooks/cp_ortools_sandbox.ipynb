{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or-tools Implementation Development Sandbox\n",
    "The goal of this notebook is to establish an effective workflow for model engineering. \n",
    "\n",
    "  - Data Read\n",
    "  - Preprocessing\n",
    "    - geocoding\n",
    "    - distance processing\n",
    "    - clustering\n",
    "    - additional configuration\n",
    "  - Modeling\n",
    "  - Post-processing\n",
    "    - Scoring\n",
    "    \n",
    "Each of these components will be fleshed out from this notebook and refactored into a newer version of the pyords library. The current workflow is as follow:\n",
    "\n",
    "1. init notebook\n",
    "2. script task\n",
    "3. test task\n",
    "4. complete objective\n",
    "5. refactor into library using the same (if not more) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ortools --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as pe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "this_dir = os.path.abspath('')\n",
    "root_dir = os.path.dirname(this_dir)\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading input shipment data\n",
    "Geocoding has been completed already. For initial versions scope is limited to contiguous US 5-digit zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../tests/vrp_testing_data.csv')\n",
    "\n",
    "required_cols = ['weight', 'pallets', 'zipcode']\n",
    "for col in required_cols: assert col in df.columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(dataframe:pd.DataFrame, colors:str=None):\n",
    "    return pe.scatter_geo(\n",
    "        dataframe, \n",
    "        lat='latitude', \n",
    "        lon='longitude', \n",
    "        size='pallets',\n",
    "        color=colors,\n",
    "        scope='usa'\n",
    "    )\n",
    "\n",
    "get_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Distance processing\n",
    "For the ortools model setup the program needs to be fed a distance matrix that includes an origin node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "\n",
    "def get_distance_matrix_from_dataframe(origins:list, dataframe:pd.DataFrame):\n",
    "    # select an origin node\n",
    "\n",
    "    distances = []\n",
    "    for coords0 in origins + list(zip(dataframe.latitude, dataframe.longitude)):\n",
    "        row = []\n",
    "        for coords1 in origins + list(zip(df.latitude, df.longitude)):\n",
    "            distance = haversine(coords0, coords1, unit=Unit.MILES)\n",
    "            row.append(distance)\n",
    "        distances.append(row)\n",
    "        \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "As part of the modeling process we'll improve how we are preparing the data. To nail down effective logic to implement, I'll look to add processing here for **clustering** and **cluster improvements**.\n",
    "By clustering we can segment the problem up into realistic problem spaces. So far DBSCAN with ad-hoc mile-constrained chaining is used to pluck out nodes close enough together to even consider for one route (TODO: needs improvement).\n",
    "Once we have these clusters we can allow for additional flexibility in the overall solution by tweaking the segmentation determined by DBSCAN. Setting a fixed constraint of X miles might work initially, but there could be return routes, connecting stops, etc. that make one-offs more appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or-tools modeling\n",
    "At this point the goal is to utilize ortools' cvrp wrappers. We are going to design this as a capacitated vrp without time windows or any complex penalties for the initial implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "\n",
    "# inputs\n",
    "# - distance matrix: graph defined as matrix \n",
    "#  (point to point; len(matrix) == number of unique nodes)\n",
    "# - vehicle list: [max_cap, max_cap, max_cap] defined in units of demand\n",
    "# - demand list: [units, units, units, ... n_distances]\n",
    "# - max_solve_seconds\n",
    "# - depot_index: poisition of node in definitions to use as origin node\n",
    "\n",
    "# process vehicles input using total destination nodes count\n",
    "# i.e. one truck available per stop\n",
    "\n",
    "\n",
    "# constructing the model\n",
    "def get_manager(distances:list, vehicles:list, depot_index:int): \n",
    "    return pywrapcp.RoutingIndexManager(len(distances), len(vehicles), 0)\n",
    "\n",
    "def get_model(manager):\n",
    "    return pywrapcp.RoutingModel(manager)\n",
    "\n",
    "# config for optimization search\n",
    "def get_search_params(max_solve_seconds:int=30):\n",
    "    search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_parameters.first_solution_strategy = \\\n",
    "        routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
    "    search_parameters.time_limit.seconds = max_solve_seconds\n",
    "    \n",
    "    return search_parameters\n",
    "\n",
    "class MyPipe:\n",
    "    def __init__(self, manager, model):\n",
    "        self.manager = manager\n",
    "        self.model = model\n",
    "        self.distances = None\n",
    "        self.vehicles = None\n",
    "        self.demand = None\n",
    "        \n",
    "    def distance_callback(self, i:int, j:int):\n",
    "        \"\"\"index of from (i) and to (j)\"\"\"\n",
    "        node_i = self.manager.IndexToNode(i)\n",
    "        node_j = self.manager.IndexToNode(j)\n",
    "\n",
    "        return self.distances[node_i][node_j]\n",
    "    \n",
    "    def set_distance_callback(self):\n",
    "        self.model.SetArcCostEvaluatorOfAllVehicles(\n",
    "            self.model.RegisterTransitCallback(self.distance_callback)\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_distances(self, distances):\n",
    "        self.distances = distances\n",
    "        self.set_distance_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def demand_callback(self, i:int):\n",
    "        \"\"\"capacity constraint\"\"\"\n",
    "        node = self.manager.IndexToNode(i)\n",
    "\n",
    "        return self.demand[node]\n",
    "    \n",
    "    def set_demand_callback(self):\n",
    "        # add demand constraint using vehicles\n",
    "        self.model.AddDimensionWithVehicleCapacity(\n",
    "            # function which return the load at each location (cf. cvrp.py example)\n",
    "            self.model.RegisterUnaryTransitCallback(self.demand_callback),\n",
    "            0, # null capacity slack\n",
    "            np.array([cap for cap in self.vehicles]), # vehicle maximum capacity\n",
    "            True, # start cumul to zero\n",
    "            'Capacity'\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_vehicles(self, vehicles:list):\n",
    "        self.vehicles = vehicles\n",
    "        if self.demand: \n",
    "            return self.set_demand_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_demand(self, demand:list):\n",
    "        self.demand = demand\n",
    "        if self.vehicles: \n",
    "            return self.set_demand_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_solution(self, assignment):\n",
    "        total_distance = 0\n",
    "        total_load = 0\n",
    "        solution = []\n",
    "        for vehicle in range(len(self.vehicles)):\n",
    "            i = self.model.Start(vehicle)\n",
    "            info = {'vehicle': vehicle, 'stops': list(), 'stop_distances': [0],\n",
    "                    'stop_loads': list()}\n",
    "\n",
    "            while not self.model.IsEnd(i):\n",
    "                node = self.manager.IndexToNode(i)\n",
    "                info['stops'].append(node)\n",
    "                info['stop_loads'].append(self.demand[node])\n",
    "\n",
    "                previous_i = int(i)\n",
    "                i = assignment.Value(self.model.NextVar(i))\n",
    "                info['stop_distances'].append(self.model.GetArcCostForVehicle(previous_i, i, vehicle))\n",
    "\n",
    "            # add return to depot to align with solution data\n",
    "            info['stops'].append(0)\n",
    "            info['stop_loads'].append(0)\n",
    "            solution.append(info)\n",
    "        \n",
    "        return solution\n",
    "            \n",
    "    def run(self, search_params):\n",
    "        return self.model.SolveWithParameters(search_params)\n",
    "    \n",
    "def get_solution_from_dataframe(dataframe:pd.DataFrame):\n",
    "    # TODO: abstraction & testing\n",
    "    origins = [(41.4191, -87.7748)] # assumed depot location for one-depot solutions\n",
    "    distances = get_distance_matrix_from_dataframe(origins, dataframe)\n",
    "    assert len(distances) == len(origins) + len(dataframe)\n",
    "    \n",
    "    vehicles = [26 for i in range(len(distances[1:]))]\n",
    "    demand = np.insert(dataframe.pallets.values, 0, 0) # using pallets & adding 0 for the depot\n",
    "    max_solve_seconds = 30\n",
    "    depot_index = 0\n",
    "    \n",
    "    manager = get_manager(distances, vehicles, depot_index)\n",
    "    model = get_model(manager)\n",
    "    pipe = MyPipe(manager, model)\n",
    "    search_params = get_search_params()\n",
    "    assignment = pipe.add_distances(distances)\\\n",
    "        .add_vehicles(vehicles)\\\n",
    "        .add_demand(demand)\\\n",
    "        .run(search_params)\n",
    "    \n",
    "    return pipe.get_solution(assignment)\n",
    "        \n",
    "test = get_solution_from_dataframe(df)\n",
    "\n",
    "assert len(test) > 0 # TODO: create better solution testing\n",
    "\n",
    "vehicleindex_w_moststops = np.argmax([len(v['stops']) for v in test])\n",
    "vehicles_w_loads = [v for v in test if sum(v['stop_loads']) > 0]\n",
    "print('total vehicles: %s' % len(test))\n",
    "print('total vehicles w loads: %s' % len(vehicles_w_loads))\n",
    "#print('total load: %s' % solution[-1])\n",
    "#print('total input load: %s' % demand.sum())\n",
    "print('max stop sequence: %s' % test[vehicleindex_w_moststops]['stops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "**Scoring** a solution against a standardized formula/method will allow for more comprehensive testing, debugging, and model tuning. Scoring should be broken down into a **theory score** and a **practical score**. Theory scores will utilize theoretical expectations of vrp solutions (some assumptions about implementations needs to be made). The practical score will measure how implementable a solution is with respect to certain real-world expectations.  \n",
    "\n",
    "For the sake of simplicity I'll use pandas as my target format and align scoring to get functions that pull from standard solution structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CheckerStage:\n",
    "    def __init__(self, solution, dataframe:pd.DataFrame):\n",
    "        self.solution = solution\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    # scoring theoretical\n",
    "    # average capacity utilization of vehicles\n",
    "    def get_load_factor(self):\n",
    "        total_loads = sum([sum(s['stop_loads']) for s in self.solution])\n",
    "        total_utilized_vehicles = len([s for s in self.solution if len(s['stops'][1:-1]) > 0])\n",
    "\n",
    "        return total_loads/total_utilized_vehicles\n",
    "\n",
    "    def score_load_factor(self):\n",
    "        return self.dataframe.groupby('vehicle').pallets.sum().mean()\n",
    "\n",
    "    # average distance traveled per vehicle\n",
    "    # NOTE: excluding distances returning to depot for now\n",
    "    # need to refactor for this.\n",
    "    def get_distance_factor(self):\n",
    "        total_distances = sum([sum(s['stop_distances'][:-1]) for s in self.solution])\n",
    "        total_utilized_vehicles = len([s for s in self.solution if len(s['stops'][1:-1]) > 0])\n",
    "\n",
    "        return total_distances/total_utilized_vehicles\n",
    "\n",
    "    def score_distance_factor(self):\n",
    "        return self.dataframe.groupby('vehicle').stop_distance.sum().mean()\n",
    "\n",
    "    # average distance per stop\n",
    "    def score_travel_factor(self):\n",
    "        return None\n",
    "\n",
    "    # ratio of one-stop routes to multi-stop \n",
    "    # (assumption is that implementation is looking for multi-stops)\n",
    "    def score_multistop_factor(self):\n",
    "        return None\n",
    "\n",
    "    # general service measured in total capacity serviced over total in scope\n",
    "    def score_multistop_factor(self):\n",
    "        return None\n",
    "\n",
    "    # scoring practice\n",
    "    # deviation/distribution of stop distances per route\n",
    "    def score_erratic_distance_factor(self):\n",
    "        return None\n",
    "\n",
    "    # measuring total number of moves across state boarders\n",
    "    def score_state_crossing_factor(self):\n",
    "        return None\n",
    "    \n",
    "    def get_df_info(self):\n",
    "        return {\n",
    "            'load_factor': self.score_load_factor(),\n",
    "            'distance_factor': self.score_distance_factor(),\n",
    "            'stop_travel_factor': self.score_travel_factor(),\n",
    "            'multistop_factor': self.score_multistop_factor(),\n",
    "            'satisfaction_factor': self.score_multistop_factor(),\n",
    "            'erratic_distance_factor': self.score_erratic_distance_factor(),\n",
    "            'crossstate_factor': self.score_state_crossing_factor()\n",
    "        }\n",
    "    \n",
    "    def psuedo_test(self):\n",
    "        info = self.get_df_info()\n",
    "        \n",
    "        assert info['load_factor'] == self.get_load_factor()\n",
    "        assert info['distance_factor'] == self.get_distance_factor()\n",
    "\n",
    "def process_solution_to_dataframe(solution:list, dataframe:pd.DataFrame):\n",
    "    for v in solution:\n",
    "        # accounting for insert of origin to matrix input\n",
    "        stops = list(np.array(v['stops'][1:-1]) - 1)\n",
    "\n",
    "        dataframe.loc[stops, 'vehicle'] = str(v['vehicle'])\n",
    "        dataframe.loc[stops, 'sequence'] = list(range(len(stops))) # assumes order matches\n",
    "        dataframe.loc[stops, 'stop_distance'] = v['stop_distances'][1:-1]\n",
    "        dataframe.loc[stops, 'stop_loads'] = v['stop_loads'][1:-1]\n",
    "    \n",
    "    return dataframe        \n",
    "\n",
    "solution = get_solution_from_dataframe(df)\n",
    "df = process_solution_to_dataframe(solution, df)\n",
    "\n",
    "checks = CheckerStage(solution, df)\n",
    "checks.psuedo_test()\n",
    "print(checks.get_df_info())\n",
    "\n",
    "get_plot(df, 'vehicle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using clusters to reduce node pools\n",
    "using dbscan we can select a chain-like cluster of nodes based on logic abstraction such as euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, x, y, epsilon=0.5, minpts=2, viz=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.minpts = minpts\n",
    "        self.viz = viz\n",
    "\n",
    "    def to_dict(self):\n",
    "        _dict = {'epsilon': self.epsilon, 'minpts': self.minpts}\n",
    "        try:\n",
    "            _dict['n X'] = len(self.X)\n",
    "        except:\n",
    "            logging.warning('X has not been set.')\n",
    "        return _dict\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.X = list(zip(x, y))\n",
    "        self.clusters = np.zeros(len(self.X), dtype=np.int32)\n",
    "        if self.viz:\n",
    "            self.viz.x = x\n",
    "            self.viz.y = y\n",
    "            self.viz.update(self.clusters)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_neighbors(X, i, epsilon):\n",
    "        neighbors = []\n",
    "        for j in range(0, len(X)):\n",
    "            a = np.array(X[i])\n",
    "            b = np.array(X[j])\n",
    "            if np.linalg.norm(a-b) < epsilon:\n",
    "                neighbors.append(j)\n",
    "        return neighbors\n",
    "\n",
    "    def build_cluster(self, i, neighbors, cluster):\n",
    "        self.clusters[i] = cluster\n",
    "        for j in neighbors:\n",
    "            if self.clusters[j] == -1:\n",
    "                self.clusters[j] = cluster\n",
    "            elif self.clusters[j] == 0:\n",
    "                self.clusters[j] = cluster\n",
    "                points = self.get_neighbors(self.X, j, self.epsilon)\n",
    "                if len(points) >= self.minpts:\n",
    "                    neighbors += points\n",
    "\n",
    "    def cluster(self, x=None, y=None):\n",
    "        if x is None or y is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            X = list(zip(x, y))\n",
    "\n",
    "        cluster = 0\n",
    "        for i in range(0, len(X)):\n",
    "            if not self.clusters[i] == 0:\n",
    "                continue\n",
    "            points = self.get_neighbors(X, i, self.epsilon)\n",
    "            if len(points) < self.minpts:\n",
    "                self.clusters[i] = -1\n",
    "            else:\n",
    "                cluster += 1\n",
    "                self.build_cluster(i, points, cluster)\n",
    "            if self.viz:\n",
    "                self.viz.update(self.clusters)\n",
    "\n",
    "def get_dbscan_clusters(dataframe:pd.DataFrame):\n",
    "    \"\"\"\n",
    "      weight,pallets,zipcode,latitude,longitude\n",
    "      18893,24,46168,39.6893,-86.3919\n",
    "      19599,25,46168,39.6893,-86.3919\n",
    "    \n",
    "    return df with clusters col\n",
    "    \"\"\"\n",
    "    epsilon = 0.79585 # approximate degree delta for 50 miles\n",
    "    minpts = 2 # at least cluster 2\n",
    "\n",
    "    # simplify euclidean distance calculation by projecting to positive vals\n",
    "    x = dataframe.latitude.values + 90\n",
    "    y = dataframe.longitude.values + 180\n",
    "\n",
    "    dbscan = DBSCAN(epsilon, minpts)\n",
    "    dbscan.fit(x, y)\n",
    "    dbscan.cluster()\n",
    "\n",
    "    return dbscan.clusters\n",
    "\n",
    "df_dbscan = df.drop(columns=['vehicle']).copy()\n",
    "df_dbscan['cluster'] = get_dbscan_clusters(df_dbscan)\n",
    "\n",
    "results = pd.DataFrame(columns=df_dbscan.columns.tolist())\n",
    "\n",
    "# TODO: optimize\n",
    "for cluster in df_dbscan.cluster.unique():\n",
    "    clustered_df = df_dbscan[df_dbscan.cluster == cluster].copy().reset_index(drop=True)\n",
    "    solution = get_solution_from_dataframe(clustered_df)\n",
    "    clustered_df = process_solution_to_dataframe(solution, clustered_df)\n",
    "    clustered_df.vehicle = str(int(cluster)) + '-' + clustered_df.vehicle.astype(int)\\\n",
    "        .astype(str)\n",
    "    results = results.append(clustered_df, sort=False)\n",
    "\n",
    "results.pallets = results.pallets.astype(int)\n",
    "    \n",
    "df_dbscan = results\n",
    "dbscan_checks = CheckerStage(solution=None, dataframe=df_dbscan)\n",
    "print(dbscan_checks.get_df_info())\n",
    "get_plot(df_dbscan, 'vehicle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbscan_checks.get_df_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checks.get_df_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
