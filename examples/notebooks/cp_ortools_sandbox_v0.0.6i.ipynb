{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or-tools Implementation Development Sandbox\n",
    "The goal of this notebook is to establish an effective workflow for model engineering. \n",
    "\n",
    "  - Data Read\n",
    "  - Preprocessing\n",
    "    - geocoding\n",
    "    - distance processing\n",
    "    - clustering\n",
    "    - additional configuration\n",
    "  - Modeling\n",
    "  - Post-processing\n",
    "    - Scoring\n",
    "    \n",
    "Each of these components will be fleshed out from this notebook and refactored into a newer version of the pyords library. The current workflow is as follow:\n",
    "\n",
    "1. init notebook\n",
    "2. script task\n",
    "3. test task\n",
    "4. complete objective\n",
    "5. refactor into library using the same (if not more) tests\n",
    "\n",
    "## Feature Implementations\n",
    "The goal is to design the implementations of each feature as simple as possible but allow for clear package utilization. As each feature is utilized they will be improved upon. Each implementation should have benefits documented for its support:\n",
    "\n",
    "\n",
    "### Distance Processing\n",
    "For Ortools integration, distance matricies define most of the model setup. Initially every model will be limited to inputs of only destinations that'd be serviced by a given, single origin. To use the function, pass the origin(s) (NOTE: pyords ortools integration will be limited to one origin node designs) and destinations to that returns a position-defined network of nodes using straight-line distances from lats and lons. This function can serve as one input implementation for ortools integration.\n",
    "\n",
    "I'm just going to add the function as is to the library within the distance namespace. No embedded implementation for this distance processing.\n",
    "\n",
    "### Cluster Processing\n",
    "DBSCAN is a recognized clustering method. A DBSCAN class can implement the more pure implementations of this clustering in pyords. The one added customization piece to this feature is the ability to expand clusters (and in turn problem space). This function can be implemented as a stand-alone branch of the library for the future of its cluster toolset.\n",
    "\n",
    "I'll add this as a DBSCAN class under the cluster namespace to keep it simple. I will add an adjacent function for processing closest clusters (TODO: nearest neighbor optimization). An embedded implementation function using each of the cluster processing methods here will be added to the namespace as well.\n",
    "\n",
    "### Ortools Modeling\n",
    "Typically ortools is utilized in script-like programs. This means that your enviroment is restricted to the purpose of your notebook. In order to effectively partition the integration from the enviroment the integration may be used as par of, pyords can provide light wrappers for the existing ortools wrappers. These wrappers will aid in task-like processing for ortools models. Testing can be done by class and allow for embedded statistics early on in the development of this library (NOTE: pyords is developed alongside any project requiring certain dependencies; [see cvrp-app](https://github.com/christopherpryer/cvrp-app)).\n",
    "\n",
    "**OrToolsPipe** will be added for classed instances of ortools managers, models, and related data to orchestrate the optimization wrappers.\n",
    "**OrToolsChecksStage** will be added for solution & dataframe cross-validation testing & reporting.\n",
    "\n",
    "TODO: create OrToolsTask\n",
    "\n",
    "There will be an embedded implementation function for this feature.\n",
    "\n",
    "### Post-processing\n",
    "TODO: shipment optimization post-processing can be added to the library after it's been abstracted from ortools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as pe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading input shipment data\n",
    "Geocoding has been completed already. For initial versions scope is limited to contiguous US 5-digit zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../tests/vrp_testing_data.csv')\n",
    "\n",
    "required_cols = ['weight', 'pallets', 'zipcode']\n",
    "for col in required_cols: assert col in df.columns\n",
    "    \n",
    "def get_plot(dataframe:pd.DataFrame, colors:str=None):\n",
    "    return pe.scatter_geo(\n",
    "        dataframe, \n",
    "        lat='latitude', \n",
    "        lon='longitude', \n",
    "        size='pallets',\n",
    "        color=colors,\n",
    "        scope='usa'\n",
    "    )\n",
    "\n",
    "get_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Distance processing\n",
    "For the ortools model setup the program needs to be fed a distance matrix that includes an origin node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "\n",
    "def create_distances_matrix(origins:list, latitudes:list, longitudes:list):\n",
    "    distances = []\n",
    "    destinations = list(zip(latitudes, longitudes))\n",
    "    for coords0 in origins + destinations:\n",
    "        row = []\n",
    "        for coords1 in origins + destinations:\n",
    "            distance = haversine(coords0, coords1, unit=Unit.MILES)\n",
    "            row.append(distance)\n",
    "        distances.append(row)\n",
    "        \n",
    "    return distances\n",
    "\n",
    "def test_create_distances_matrix():\n",
    "    origins = [(0, 1)]\n",
    "    latitudes = [1, 2, 3, 4]\n",
    "    longitudes = [1, 2, 3, 4]\n",
    "    matrix = create_distances_matrix(origins, latitudes, longitudes)\n",
    "    \n",
    "    assert len(matrix) == len(origins) + len(latitudes)\n",
    "    \n",
    "test_create_distances_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "As part of the modeling process we'll improve how we are preparing the data. To nail down effective logic to implement, I'll look to add processing here for **clustering** and **cluster improvements**.\n",
    "By clustering we can segment the problem up into realistic problem spaces. So far DBSCAN with ad-hoc mile-constrained chaining is used to pluck out nodes close enough together to even consider for one route (TODO: needs improvement).\n",
    "Once we have these clusters we can allow for additional flexibility in the overall solution by tweaking the segmentation determined by DBSCAN. Setting a fixed constraint of X miles might work initially, but there could be return routes, connecting stops, etc. that make one-offs more appealing.\n",
    "\n",
    "#### using clusters to reduce node pools\n",
    "using dbscan we can select a chain-like cluster of nodes based on logic abstraction such as euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, x, y, epsilon=0.5, minpts=2, viz=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.minpts = minpts\n",
    "        self.viz = viz\n",
    "\n",
    "    def to_dict(self):\n",
    "        _dict = {'epsilon': self.epsilon, 'minpts': self.minpts}\n",
    "        try:\n",
    "            _dict['n X'] = len(self.X)\n",
    "        except:\n",
    "            logging.warning('X has not been set.')\n",
    "        return _dict\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.X = list(zip(x, y))\n",
    "        self.clusters = np.zeros(len(self.X), dtype=np.int32)\n",
    "        if self.viz:\n",
    "            self.viz.x = x\n",
    "            self.viz.y = y\n",
    "            self.viz.update(self.clusters)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_neighbors(X, i, epsilon):\n",
    "        neighbors = []\n",
    "        for j in range(0, len(X)):\n",
    "            a = np.array(X[i])\n",
    "            b = np.array(X[j])\n",
    "            if np.linalg.norm(a-b) < epsilon:\n",
    "                neighbors.append(j)\n",
    "        return neighbors\n",
    "\n",
    "    def build_cluster(self, i, neighbors, cluster):\n",
    "        self.clusters[i] = cluster\n",
    "        for j in neighbors:\n",
    "            if self.clusters[j] == -1:\n",
    "                self.clusters[j] = cluster\n",
    "            elif self.clusters[j] == 0:\n",
    "                self.clusters[j] = cluster\n",
    "                points = self.get_neighbors(self.X, j, self.epsilon)\n",
    "                if len(points) >= self.minpts:\n",
    "                    neighbors += points\n",
    "\n",
    "    def cluster(self, x=None, y=None):\n",
    "        if x is None or y is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            X = list(zip(x, y))\n",
    "\n",
    "        cluster = 0\n",
    "        for i in range(0, len(X)):\n",
    "            if not self.clusters[i] == 0:\n",
    "                continue\n",
    "            points = self.get_neighbors(X, i, self.epsilon)\n",
    "            if len(points) < self.minpts:\n",
    "                self.clusters[i] = -1\n",
    "            else:\n",
    "                cluster += 1\n",
    "                self.build_cluster(i, points, cluster)\n",
    "            if self.viz:\n",
    "                self.viz.update(self.clusters)\n",
    "\n",
    "def add_closest_clusters(x:list, y:list, clusters:list):\n",
    "    \"\"\"\n",
    "    Takes a list of x, a list of y, and a list of clusters to\n",
    "    process clusters for x, y without an assigned cluster.\n",
    "    To accomplish this the function calculates the euclidean\n",
    "    distance to every node with a cluster. It will then assign\n",
    "    the found node's cluster as its own.\n",
    "    \n",
    "    x: list-like of x coordinates\n",
    "    y: list-like of y coordinates\n",
    "    clusters: list-like of clusters assigned\n",
    "    \n",
    "    return list of clusters\n",
    "    \"\"\"\n",
    "    c = list(clusters)\n",
    "    \n",
    "    positions = list(range(len(c)))\n",
    "    missing_clusters = [i for i in positions if pd.isnull(c[i])]\n",
    "    has_clusters = [i for i in positions if i not in missing_clusters]\n",
    "    \n",
    "    x_copy = np.array(x, dtype=float)\n",
    "    x_copy[missing_clusters] = np.inf\n",
    "\n",
    "    y_copy = np.array(y, dtype=float)\n",
    "    y_copy[missing_clusters] = np.inf\n",
    "    \n",
    "    for i in missing_clusters:\n",
    "        x_deltas = abs(x[i] - x_copy)\n",
    "        y_deltas = abs(y[i] - y_copy)\n",
    "        deltas = x_deltas + y_deltas\n",
    "        \n",
    "        c[i] = c[np.argmin(deltas)]\n",
    "    \n",
    "    return c\n",
    "\n",
    "def create_dbscan_basic(x:list, y:list):\n",
    "    epsilon = 0.79585 # approximate degree delta for 50 miles\n",
    "    minpts = 2 # at least cluster 2\n",
    "\n",
    "    dbscan = DBSCAN(epsilon, minpts)\n",
    "    dbscan.fit(x, y)\n",
    "    dbscan.cluster()\n",
    "    \n",
    "    return dbscan\n",
    "                \n",
    "def create_dbscan_expanded_clusters(x:list, y:list):   \n",
    "    dbscan = create_dbscan_basic(x, y)\n",
    "    \n",
    "    # add those without an assigned cluster\n",
    "    # to their closest cluster\n",
    "    clusters = [c if int(c) >= 0 else None for c in dbscan.clusters]\n",
    "    clusters = add_closest_clusters(x, y, clusters)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def test_add_closest_clusters():\n",
    "    old_clusters = [1, 2, 3, None]\n",
    "    new_clusters = add_closest_clusters([1, 2, 3, 4], [1, 2, 3, 4], old_clusters)\n",
    "    \n",
    "    assert len(old_clusters) == len(new_clusters)\n",
    "    assert old_clusters != new_clusters\n",
    "    assert new_clusters[-1] == 3 #hardcoded\n",
    "\n",
    "def test_create_dbscan_basic():\n",
    "    x, y = [1, 2, 3], [1, 2, 3]\n",
    "    tdbscan = create_dbscan_basic(x, y)\n",
    "    \n",
    "    assert len(tdbscan.clusters) > 0\n",
    "    assert len(tdbscan.clusters) == len(x)\n",
    "    \n",
    "def test_create_dbscan_expanded_clusters():\n",
    "    x, y = [1, 2, 3], [1, 2, 3]\n",
    "    clusters = create_dbscan_expanded_clusters(x, y)\n",
    "    \n",
    "    assert len(clusters) > 0\n",
    "    assert len(clusters) == len(x)\n",
    "\n",
    "test_add_closest_clusters()\n",
    "test_create_dbscan_basic()\n",
    "test_create_dbscan_expanded_clusters()\n",
    "\n",
    "# simplify euclidean distance calculation by projecting to positive vals\n",
    "x = df.latitude.values + 90\n",
    "y = df.longitude.values + 180\n",
    "\n",
    "# pyords cluster implementation\n",
    "df['cluster'] = create_dbscan_expanded_clusters(x, y)\n",
    "df.cluster = df.cluster.astype(str)\n",
    "get_plot(df, 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or-tools Modeling\n",
    "At this point the goal is to utilize ortools' cvrp wrappers. We are going to design this as a capacitated vrp without time windows or any complex penalties for the initial implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "\n",
    "# inputs\n",
    "# - distance matrix: graph defined as matrix \n",
    "#  (point to point; len(matrix) == number of unique nodes)\n",
    "# - vehicle list: [max_cap, max_cap, max_cap] defined in units of demand\n",
    "# - demand list: [units, units, units, ... n_distances]\n",
    "# - max_solve_seconds\n",
    "# - depot_index: poisition of node in definitions to use as origin node\n",
    "\n",
    "# process vehicles input using total destination nodes count\n",
    "# i.e. one truck available per stop\n",
    "class OrToolsTask:\n",
    "    def __init__(self, manager, model):\n",
    "        self.manager = manager\n",
    "        self.model = model\n",
    "        self.distances = None\n",
    "        self.vehicles = None\n",
    "        self.demand = None\n",
    "        \n",
    "    def distance_callback(self, i:int, j:int):\n",
    "        \"\"\"index of from (i) and to (j)\"\"\"\n",
    "        node_i = self.manager.IndexToNode(i)\n",
    "        node_j = self.manager.IndexToNode(j)\n",
    "\n",
    "        return self.distances[node_i][node_j]\n",
    "    \n",
    "    def set_distance_callback(self):\n",
    "        self.model.SetArcCostEvaluatorOfAllVehicles(\n",
    "            self.model.RegisterTransitCallback(self.distance_callback)\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_distances(self, distances):\n",
    "        self.distances = distances\n",
    "        self.set_distance_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def demand_callback(self, i:int):\n",
    "        \"\"\"capacity constraint\"\"\"\n",
    "        node = self.manager.IndexToNode(i)\n",
    "\n",
    "        return self.demand[node]\n",
    "    \n",
    "    def set_demand_callback(self):\n",
    "        # add demand constraint using vehicles\n",
    "        self.model.AddDimensionWithVehicleCapacity(\n",
    "            # function which return the load at each location (cf. cvrp.py example)\n",
    "            self.model.RegisterUnaryTransitCallback(self.demand_callback),\n",
    "            0, # null capacity slack\n",
    "            np.array([cap for cap in self.vehicles]), # vehicle maximum capacity\n",
    "            True, # start cumul to zero\n",
    "            'Capacity'\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_vehicles(self, vehicles:list):\n",
    "        self.vehicles = vehicles\n",
    "        if self.demand: \n",
    "            return self.set_demand_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_demand(self, demand:list):\n",
    "        self.demand = demand\n",
    "        if self.vehicles: \n",
    "            return self.set_demand_callback()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_solution(self, assignment):\n",
    "        total_distance = 0\n",
    "        total_load = 0\n",
    "        solution = []\n",
    "        for vehicle in range(len(self.vehicles)):\n",
    "            i = self.model.Start(vehicle)\n",
    "            info = {'vehicle': vehicle, 'stops': list(), 'stop_distances': [0],\n",
    "                    'stop_loads': list()}\n",
    "\n",
    "            while not self.model.IsEnd(i):\n",
    "                node = self.manager.IndexToNode(i)\n",
    "                info['stops'].append(node)\n",
    "                info['stop_loads'].append(self.demand[node])\n",
    "\n",
    "                previous_i = int(i)\n",
    "                i = assignment.Value(self.model.NextVar(i))\n",
    "                info['stop_distances'].append(self.model.GetArcCostForVehicle(previous_i, i, vehicle))\n",
    "\n",
    "            # add return to depot to align with solution data\n",
    "            info['stops'].append(0)\n",
    "            info['stop_loads'].append(0)\n",
    "            solution.append(info)\n",
    "        \n",
    "        return solution\n",
    "            \n",
    "    def run(self, search_params):\n",
    "        return self.model.SolveWithParameters(search_params)\n",
    "    \n",
    "# constructing the model\n",
    "def create_manager(n_nodes:int, n_vehicles:int, depot_index:int): \n",
    "    return pywrapcp.RoutingIndexManager(n_nodes, n_vehicles, 0)\n",
    "\n",
    "def create_model(initialized_manager):\n",
    "    return pywrapcp.RoutingModel(initialized_manager)\n",
    "\n",
    "# config for optimization search\n",
    "def create_search_params(max_solve_seconds:int=30):\n",
    "    search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_parameters.first_solution_strategy = \\\n",
    "        routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
    "    search_parameters.time_limit.seconds = max_solve_seconds\n",
    "    \n",
    "    return search_parameters\n",
    "\n",
    "# implementation\n",
    "def create_origins_input():\n",
    "    return [(41.4191, -87.7748)]\n",
    "    \n",
    "def create_distances_input(latitudes:list, longitudes:list):\n",
    "    # TODO: abstraction & testing\n",
    "    origins = create_origins_input() # assumed depot location for one-depot solutions\n",
    "    distances = create_distances_matrix(origins, latitudes, longitudes)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def create_vehicles_input(distances:list, m:int=26):\n",
    "    # build vehicles of m cap by len(distances[1:])\n",
    "    return [m for i in range(len(distances[1:]))] \n",
    "\n",
    "def create_demand_input(loads:np.array):\n",
    "    # unit is load for each node with demand (in this case\n",
    "    # only destinations). inserting 0 at the front of the array\n",
    "    return np.insert(loads, 0, 0)\n",
    "    \n",
    "def solve_problem_w_ortools(distances:list, vehicles:list, demand:list):\n",
    "    max_solve_seconds = 30\n",
    "    depot_index = 0\n",
    "    \n",
    "    manager = create_manager(len(distances), len(vehicles), depot_index)\n",
    "    model = create_model(manager)\n",
    "    task = OrToolsTask(manager, model)\n",
    "    search_params = create_search_params()\n",
    "    assignment = task.add_distances(distances)\\\n",
    "        .add_vehicles(vehicles)\\\n",
    "        .add_demand(demand)\\\n",
    "        .run(search_params)\n",
    "    \n",
    "    return task.get_solution(assignment)\n",
    "\n",
    "def init(dataframe:pd.DataFrame):\n",
    "    lats, lons = dataframe.latitude, dataframe.longitude\n",
    "    matrix = create_distances_input(lats, lons)\n",
    "    vehicles = create_vehicles_input(matrix)\n",
    "    demand = create_demand_input(dataframe.pallets.values)\n",
    "    \n",
    "    return matrix, vehicles, demand\n",
    "\n",
    "def test_model_setup_implementation():\n",
    "    lats, lons = [1, 2, 3], [1, 2, 3]\n",
    "    origins = create_origins_input()\n",
    "    matrix = create_distances_input(lats, lons)\n",
    "    vehicles = create_vehicles_input(matrix)\n",
    "    demand = create_demand_input([1, 2, 3])\n",
    "    \n",
    "    assert len(matrix) == len(origins) + len(lats)\n",
    "    assert len(vehicles) == len(lats)\n",
    "    assert len(demand) == len(matrix)\n",
    "\n",
    "test_model_setup_implementation()\n",
    "\n",
    "matrix, vehicles, demand = init(df)\n",
    "basic_solution = solve_problem_w_ortools(matrix, vehicles, demand)\n",
    "\n",
    "assert len(basic_solution) > 0 # TODO: create better solution testing\n",
    "\n",
    "vehicleindex_w_moststops = np.argmax([len(v['stops']) for v in basic_solution])\n",
    "vehicles_w_loads = [v for v in basic_solution if sum(v['stop_loads']) > 0]\n",
    "print('test>> total vehicles: %s' % len(basic_solution))\n",
    "print('test>> total vehicles w loads: %s' % len(vehicles_w_loads))\n",
    "#print('total load: %s' % solution[-1])\n",
    "#print('total input load: %s' % demand.sum())\n",
    "print('test>> max stop sequence: %s' % basic_solution[vehicleindex_w_moststops]['stops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "**Scoring** a solution against a standardized formula/method will allow for more comprehensive testing, debugging, and model tuning. Scoring should be broken down into a **theory score** and a **practical score**. Theory scores will utilize theoretical expectations of vrp solutions (some assumptions about implementations needs to be made). The practical score will measure how implementable a solution is with respect to certain real-world expectations.  \n",
    "\n",
    "For the sake of simplicity I'll use pandas as my target format and align scoring to get functions that pull from standard solution structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class OrToolsChecks: # TODO: figure out best way to abstract checks & tests here\n",
    "    def __init__(self, solution, dataframe:pd.DataFrame):\n",
    "        self.solution = solution\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    # scoring theoretical\n",
    "    # average capacity utilization of vehicles\n",
    "    def get_load_factor(self, solution:list=None):\n",
    "        if solution is None: solution = self.solution\n",
    "            \n",
    "        total_loads = sum([sum(s['stop_loads']) for s in solution])\n",
    "        total_utilized_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "\n",
    "        return total_loads / total_utilized_vehicles\n",
    "\n",
    "    def score_load_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "                \n",
    "        return dataframe.groupby('vehicle').pallets.sum().mean()\n",
    "\n",
    "    # average distance traveled per vehicle\n",
    "    # NOTE: excluding distances returning to depot for now\n",
    "    # need to refactor for this.\n",
    "    def get_distance_factor(self, solution:list=None):\n",
    "        if solution is None: solution = self.solution\n",
    "            \n",
    "        total_distances = sum([sum(s['stop_distances'][:-1]) for s in solution])\n",
    "        total_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "\n",
    "        return total_distances / total_vehicles\n",
    "\n",
    "    def score_distance_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "            \n",
    "        return dataframe.groupby('vehicle').stop_distance.sum().mean()\n",
    "\n",
    "    def get_travel_factor(self, solution:list=None):\n",
    "        if solution is None: solution = self.solution\n",
    "            \n",
    "        total_distances = sum([sum(s['stop_distances'][:-1]) for s in solution])\n",
    "        total_stops = sum([len(s['stops'][1:-1]) for s in solution])\n",
    "\n",
    "        return total_distances / total_stops\n",
    "    \n",
    "    # average distance per stop\n",
    "    def score_travel_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "            \n",
    "        return dataframe.stop_distance.mean()\n",
    "\n",
    "    # ratio of one-stop routes to multi-stop \n",
    "    # (assumption is that implementation is looking for multi-stops)\n",
    "    def get_multistop_factor(self, solution:list=None):\n",
    "        if solution is None: solution = self.solution\n",
    "        \n",
    "        total_multistop_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 1])\n",
    "        total_vehicles = len([s for s in solution if len(s['stops'][1:-1]) > 0])\n",
    "\n",
    "        return total_multistop_vehicles / total_vehicles\n",
    "        \n",
    "    # general service measured in total capacity serviced over total in scope\n",
    "    def score_multistop_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "        vehicles = dataframe.groupby('vehicle')\n",
    "        \n",
    "        return (vehicles.size() > 1).sum() / len(vehicles)\n",
    "    \n",
    "    def get_satisfaction_factor(self, solution:list=None):\n",
    "        if solution is None: solution = self.solution\n",
    "        \n",
    "        return None # TODO: can't pull from solution alone\n",
    "    \n",
    "    def score_satisfaction_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "            \n",
    "        return dataframe.stop_load.sum() / dataframe.pallets.sum()\n",
    "\n",
    "    # scoring practice\n",
    "    # deviation/distribution of stop distances per route\n",
    "    def score_erratic_distance_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "        \n",
    "        return dataframe.stop_distance.std()\n",
    "\n",
    "    # measuring total number of moves across state boarders\n",
    "    def score_state_crossing_factor(self, dataframe:pd.DataFrame=None):\n",
    "        if dataframe is None: dataframe = self.dataframe\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def get_df_info(self):\n",
    "        return {\n",
    "            'load_factor': self.score_load_factor(),\n",
    "            'distance_factor': self.score_distance_factor(),\n",
    "            'travel_factor': self.score_travel_factor(),\n",
    "            'multistop_factor': self.score_multistop_factor(),\n",
    "            'satisfaction_factor': self.score_satisfaction_factor(),\n",
    "            'erratic_distance_factor': self.score_erratic_distance_factor(),\n",
    "            'crossstate_factor': self.score_state_crossing_factor()\n",
    "        }\n",
    "    \n",
    "    def psuedo_test(self):\n",
    "        info = self.get_df_info()\n",
    "        \n",
    "        assert info['load_factor'] == self.get_load_factor()\n",
    "        assert info['travel_factor'] == self.get_travel_factor()\n",
    "        assert info['distance_factor'] == self.get_distance_factor()\n",
    "        assert info['multistop_factor'] == self.get_multistop_factor()\n",
    "        # NOTE: other factors are impossible to pull from just the solution\n",
    "        # moving on for now.\n",
    "        \n",
    "\n",
    "def process_solution_to_dataframe(solution:list, dataframe:pd.DataFrame):\n",
    "    for v in solution:\n",
    "        # accounting for insert of origin to matrix input\n",
    "        stops = list(np.array(v['stops'][1:-1]) - 1)\n",
    "\n",
    "        dataframe.loc[stops, 'vehicle'] = str(v['vehicle'])\n",
    "        dataframe.loc[stops, 'sequence'] = list(range(len(stops))) # assumes order matches\n",
    "        dataframe.loc[stops, 'stop_distance'] = v['stop_distances'][1:-1]\n",
    "        dataframe.loc[stops, 'stop_load'] = v['stop_loads'][1:-1]\n",
    "    \n",
    "    return dataframe        \n",
    "\n",
    "basic_df = process_solution_to_dataframe(basic_solution, df)\n",
    "\n",
    "checks = OrToolsChecks(basic_solution, df)\n",
    "checks.psuedo_test()\n",
    "print(checks.get_df_info())\n",
    "\n",
    "get_plot(df, 'vehicle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dbscan_df = df.copy()\n",
    "\n",
    "results = pd.DataFrame(columns=dbscan_df.columns.tolist())\n",
    "\n",
    "# TODO: optimize\n",
    "for cluster in dbscan_df.cluster.unique():\n",
    "    clustered_df = dbscan_df[dbscan_df.cluster == cluster].copy().reset_index(drop=True)\n",
    "    \n",
    "    matrix, vehicles, demand = init(clustered_df)\n",
    "    solution = solve_problem_w_ortools(matrix, vehicles, demand)\n",
    "    clustered_df = process_solution_to_dataframe(solution, clustered_df)\n",
    "    clustered_df.vehicle = str(int(cluster)) + '-' + clustered_df.vehicle.astype(int)\\\n",
    "        .astype(str)\n",
    "    results = results.append(clustered_df, sort=False)\n",
    "\n",
    "results.pallets = results.pallets.astype(int)\n",
    "    \n",
    "dbscan_df = results\n",
    "dbscan_checks = OrToolsChecks(solution=None, dataframe=dbscan_df)\n",
    "print(dbscan_checks.get_df_info())\n",
    "get_plot(dbscan_df, 'vehicle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checks.get_df_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbscan_checks.get_df_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
